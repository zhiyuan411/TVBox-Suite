#! /usr/bin/env python3
# pip install deepmerge charset-normalizer requests
from deepmerge import Merger
import datetime
import json
import sys
import re
import requests
from pathlib import Path
from urllib.parse import urljoin  # [æ–°å¢] ç”¨äºæ ‡å‡†è·¯å¾„æ‹¼æ¥

from charset_normalizer import from_bytes


# å®šä¹‰å¸¸é‡
INPUT_FILE_PATH = "input.txt"
OUTPUT_FILE_PATH = "output.txt"
# ================= [æ–°å¢] å®šä¹‰é»˜è®¤m3uè¾“å‡ºæ–‡ä»¶å =================
DEFAULT_OUTPUT_M3U_FILE = "output-m3u.txt"
# ================= [æ–°å¢] å®šä¹‰é»˜è®¤txtè¾“å‡ºæ–‡ä»¶å =================
DEFAULT_OUTPUT_TXT_FILE = "output-txt.txt"
# ================= [æ–°å¢] å®šä¹‰é»˜è®¤è¦†ç›–æ–‡ä»¶å =================
DEFAULT_OVERRIDE_FILE = "override.json"

# ================= [æ–°å¢] å®šä¹‰ URL æ›¿æ¢æ˜ å°„ =================
URL_REPLACEMENTS = [
    {
        "old": r".*https://raw\.githubusercontent\.com",
        "new": "https://rawgithubusercontent.cnfaq.cn"
    }
]
# =========================================================

# ================= [æ–°å¢] å®šä¹‰å¤šä½™å­—æ®µåˆ—è¡¨ =================
EXTRA_FIELDS = [
    'flags', 'warningText', 'doh', 'logo', 'urls', 'notice',
    'disabled_wallpaper', 'storeHouse', 'code', 'msg', 'page', 'pagecount',
    'limit', 'total', 'list', 'class', 'iptv', 'channel', 'drive', 'analyze',
    'setting', 'analyzeHistory', 'history', 'searchHistory', 'star', 'homepage',
    'homeLogo', 'adblock', 'recommend', 'rating', 'pullWord', 'subtitle'
]
# =========================================================

# ================= [æ–°å¢] å®šä¹‰siteså¿…éœ€å­—æ®µåˆ—è¡¨ =================
SITES_REQUIRED_FIELDS = ['key', 'name', 'api', 'type']
# =========================================================

# å®šä¹‰ç”¨äºåˆ¤æ–­å•ä»“/å¤šä»“çš„ç‰¹å¾å­—æ®µåˆ—è¡¨
SINGLE_CANG_FIELDS = {'video', 'spider', 'sites', 'iptv', 'channel', 'analyze', 'lives', 'parses'}

# å®šä¹‰é¢‘é“èšåˆæ’é™¤å…³é”®å­—
CHANNEL_AGGREGATION_EXCLUDE_KEYWORDS = ['ç¬¬']

# å®šä¹‰é¢‘é“åæ¸…æ´—å…³é”®å­—
CHANNEL_NAME_CLEAN_KEYWORDS = ['-']

# å®šä¹‰åˆ†ç»„åæ¸…æ´—å…³é”®å­—
GROUP_NAME_CLEAN_KEYWORDS = ['é¢‘é“', 'ä¸¨', 'ï½œ', 'Â·', '-', '_', ';', '.', 'ğŸ“º', 'â˜˜ï¸'
, 'ğŸ€', 'ğŸ›', 'ğŸ¬', 'ğŸª', 'ğŸ‡¨ğŸ‡³', 'ğŸ‘ ', 'ğŸ’‹', 'ğŸ’ƒ', 'ğŸ’', 'ğŸ’–', 'ğŸ±', 'ğŸ›°', 'ğŸ”¥', 'ğŸ¤¹ğŸ¼'
, 'ğŸ¼', 'ğŸ“›', 'ğŸ·', 'ğŸ»', 'ğŸ’°', 'ğŸµ', 'ğŸ®', 'ğŸ“¡', 'ğŸ•˜ï¸', 'ğŸ“¢', 'ğŸ', 'ğŸŒŠ', 'ğŸ‡­ğŸ‡°', 'ğŸ‡¹ğŸ‡¼'
, 'ğŸ‡°ğŸ‡·', 'ğŸ°', 'ğŸ‡¯ğŸ‡µ', 'ğŸ“»', 'ğŸ‡ºğŸ‡¸', 'ğŸ™', 'ğŸŒ', 'ğŸ–¥', 'ğŸ“½', 'ğŸ”¥', 'ğŸ¬', 'ğŸ’°', 'ğŸ†•']

def remove_comments_from_string(input_string):
    input_string = re.sub(r'^[ ]*//[^\n]*', '', input_string, flags=re.MULTILINE)
    input_string = re.sub(r'^[ ]*#[^\n]*', '', input_string, flags=re.MULTILINE)
    input_string = re.sub(r'^[ ]*/\*.*?\*/', '', input_string, flags=re.DOTALL)
    return input_string

def preprocess_url(url):
    """
    é¢„å¤„ç†URLï¼Œæ ¹æ®URL_REPLACEMENTSè¿›è¡Œæ›¿æ¢
    :param url: åŸå§‹URL
    :return: æ›¿æ¢åçš„URL
    """
    processed_url = url
    for replacement in URL_REPLACEMENTS:
        if replacement["old"] in processed_url:
            old_url = processed_url
            processed_url = processed_url.replace(replacement["old"], replacement["new"])
            print(f"  [URL Replace] {old_url} -> {processed_url}")
    return processed_url


def is_json(content):
    try:
        json.loads(content)
    except ValueError:
        return False
    return True

def detect_encoding(byte_data):
    """
    æ£€æµ‹å­—èŠ‚æµçš„ç¼–ç  (ä½¿ç”¨ charset-normalizer)
    :param byte_data: bytes
    :return: str ç¼–ç åç§°
    """
    if not byte_data:
        return 'utf-8'

    # ä½¿ç”¨ charset-normalizer è¿›è¡Œæ£€æµ‹
    result = from_bytes(byte_data).best()

    # å¦‚æœæ£€æµ‹åˆ°ç»“æœï¼Œç›´æ¥ä½¿ç”¨å…¶ç¼–ç ï¼›å¦åˆ™é»˜è®¤ utf-8
    if result:
        return result.encoding
    return 'utf-8'

def decode_safely(byte_data):
    """
    å®‰å…¨è§£ç å­—èŠ‚æµä¸ºå­—ç¬¦ä¸²
    :param byte_data: bytes
    :return: str or None
    """
    if not byte_data:
        return None

    # 1. ç®€å•çš„äºŒè¿›åˆ¶æ–‡ä»¶æ£€æŸ¥ (ä¾‹å¦‚ PNG header 0x89504E47, JPEG header 0xFFD8FF)
    # å¦‚æœæ˜¯å›¾ç‰‡ç­‰æ˜æ˜¾çš„äºŒè¿›åˆ¶ï¼Œç›´æ¥è¿”å› None
    if len(byte_data) > 4:
        header = byte_data[:4]
        # PNG, JPEG, GIF, PDF, ZIP ç­‰å¸¸è§äºŒè¿›åˆ¶å¤´
        binary_headers = [
            b'\x89PNG', b'\xff\xd8\xff', b'GIF8', b'%PDF', b'PK\x03\x04'
        ]
        for bh in binary_headers:
            if header.startswith(bh):
                print("  [Skip] æ£€æµ‹åˆ°äºŒè¿›åˆ¶æ–‡ä»¶å¤´ï¼Œè·³è¿‡è§£ç ã€‚")
                return None

    encoding = detect_encoding(byte_data)

    try:
        # å°è¯•ç”¨æ£€æµ‹åˆ°çš„ç¼–ç è§£ç 
        return byte_data.decode(encoding)
    except (UnicodeDecodeError, LookupError):
        try:
            # å¤±è´¥åˆ™å°è¯• UTF-8 å®¹é”™
            return byte_data.decode('utf-8', errors='replace')
        except Exception:
            return None

def get_local_file_content(file_path):
    try:
        # ä»¥äºŒè¿›åˆ¶æ¨¡å¼è¯»å–
        with open(file_path, 'rb') as file:
            byte_content = file.read()
        print(f"Read local file: {file_path}")

        # è§£ç 
        content = decode_safely(byte_content)
        return content
    except Exception as e:
        print(f"Error reading local file {file_path}: {e}")
        return None

def get_url_content(url, timeout=10):
    try:
        # é¢„å¤„ç† URL
        processed_url = preprocess_url(url)
        
        response = requests.get(processed_url, timeout=timeout)
        response.raise_for_status()

        byte_content = response.content

        # æ£€æŸ¥ HTTP Content-Typeï¼Œè¿‡æ»¤æ‰æ˜æ˜¾çš„éæ–‡æœ¬
        content_type = response.headers.get('Content-Type', '').lower()
        skip_types = ['image/', 'video/', 'audio/', 'application/octet-stream', 'application/pdf', 'application/zip']
        if any(t in content_type for t in skip_types):
            print(f"  [Skip] URL Content-Type ä¸ºéæ–‡æœ¬ç±»å‹: {content_type}")
            return None

        print(f"Fetched URL: {url}")

        # è§£ç 
        content = decode_safely(byte_content)
        return content
    except requests.Timeout as e:
        print(f"Request timed out for URL {url}: {e}")
        return None
    except requests.RequestException as e:
        print(f"Error fetching URL {url}: {e}")
        return None

def append_to_file_unique(file_path, line, existing_lines=None):
    """
    å‘æ–‡ä»¶ä¸­æ·»åŠ å”¯ä¸€è¡Œ
    :param file_path: æ–‡ä»¶è·¯å¾„
    :param line: è¦æ·»åŠ çš„è¡Œ
    :param existing_lines: å·²å­˜åœ¨çš„è¡Œé›†åˆï¼ˆå¯é€‰ï¼‰
    """
    p = Path(file_path)
    
    # å¦‚æœæ²¡æœ‰æä¾› existing_linesï¼Œåˆ™è¯»å–æ–‡ä»¶
    if existing_lines is None:
        existing_lines = set()
        if p.exists():
            try:
                with open(p, 'r', encoding='utf-8') as f:
                    for l in f:
                        stripped = l.strip()
                        if stripped:
                            existing_lines.add(stripped)
            except Exception as e:
                print(f"Warning: Could not read history file {file_path}: {e}")
    
    if line not in existing_lines:
        try:
            with open(p, 'a', encoding='utf-8') as f:
                f.write(line + '\n')
            print(f"Appended to file: {line} -> {file_path.name}")
        except Exception as e:
            print(f"Error writing to file {file_path}: {e}")

def write_list_to_file(file_path, lines):
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            for line in lines:
                f.write(line + '\n')
        print(f"Written list to: {file_path}")
    except Exception as e:
        print(f"Error writing list file {file_path}: {e}")

def replace_file(source_path, target_path):
    try:
        src = Path(source_path)
        tgt = Path(target_path)
        if src.exists():
            src.replace(tgt)
            print(f"Replaced original input file {target_path} with valid list.")
    except Exception as e:
        print(f"Error replacing file: {e}")

def is_single_cang(parsed_json):
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºå•ä»“
    """
    if not isinstance(parsed_json, dict):
        return False

    top_level_keys = set(parsed_json.keys())
    if top_level_keys & SINGLE_CANG_FIELDS:
        return True

    return False

def extract_urls_deep(obj):
    """
    æ·±åº¦éå† JSON å¯¹è±¡ï¼Œæå–æ‰€æœ‰ä»¥ http(s):// å¼€å¤´çš„å­—ç¬¦ä¸²
    """
    urls = []
    if isinstance(obj, str):
        if obj.startswith(('http://', 'https://')):
            urls.append(obj)
    elif isinstance(obj, dict):
        for value in obj.values():
            urls.extend(extract_urls_deep(value))
    elif isinstance(obj, list):
        for item in obj:
            urls.extend(extract_urls_deep(item))
    return urls

def fetch_and_parse_single_cang(url):
    """
    å°è¯•è·å–ä¸€ä¸ª URL å¹¶å°†å…¶è§£æä¸ºå•ä»“æ•°æ®
    """
    print(f"  [Multi->Single] Fetching sub-url: {url}")
    content = None

    if url.startswith('/') or url.startswith('.'):
        content = get_local_file_content(url)
    elif url.startswith('http'):
        content = get_url_content(url)
    else:
        return None

    if content is None:
        return None

    # æ¸…æ´—
    content = remove_comments_from_string(content)
    content = content.replace("\n", "").replace("\r", "")

    if not is_json(content):
        return None

    try:
        parsed = json.loads(content)
        if isinstance(parsed, dict):
            return parsed
        else:
            return None
    except:
        return None

def process_input_file(input_file_path=INPUT_FILE_PATH):
    """
    å¤„ç†è¾“å…¥æ–‡ä»¶
    """
    raw_data_map = {} # å­˜å‚¨åŸå§‹æ•°æ®: url -> data
    valid_sources = []
    invalid_sources = []

    try:
        with open(input_file_path, 'r', encoding='utf-8') as input_file:
            for line in input_file:
                trimmed_line = line.strip()
                if not trimmed_line:
                    continue

                print(f"Processing line: {trimmed_line}")
                content = None

                if trimmed_line.startswith('/') or trimmed_line.startswith('.'):
                    content = get_local_file_content(trimmed_line)
                elif trimmed_line.startswith('http'):
                    content = get_url_content(trimmed_line)
                else:
                    print("Line does not start with '/' or 'http', skipping.")
                    invalid_sources.append(trimmed_line)
                    continue

                if content is not None:
                    content = remove_comments_from_string(content)
                    content = content.replace("\n", "").replace("\r", "")

                if content is not None and is_json(content):
                    try:
                        parsed_dict = json.loads(content)
                        raw_data_map[trimmed_line] = parsed_dict
                        valid_sources.append(trimmed_line)
                        print("Parsed JSON successfully.")
                    except Exception as e:
                        print(f"JSON è§£æå¤±è´¥: {e}")
                        invalid_sources.append(trimmed_line)
                else:
                    print("Content is not valid JSON, skipping.")
                    invalid_sources.append(trimmed_line)

        return raw_data_map, valid_sources, invalid_sources
    except FileNotFoundError:
        print(f"The file {input_file_path} was not found.")
        return {}, [], []
    except Exception as e:
        print(f"An error occurred while processing the file: {e}")
        return {}, [], []


def custom_list_merge(merger, path, list1, list2):
    if all(isinstance(item, dict) for item in list1 + list2):
        key_id_dict = {}
        identifier_fields = ['key', 'id', 'name']

        for item in list1:
            identifier = next((item.get(field) for field in identifier_fields if item.get(field)), None)
            if identifier is not None:
                key_id_dict[identifier] = item
            else:
                key_id_dict[id(item)] = item

        for item in list2:
            identifier = next((item.get(field) for field in identifier_fields if item.get(field)), None)
            if identifier is not None:
                if identifier in key_id_dict:
                    key_id_dict[identifier].update(item)
                else:
                    key_id_dict[identifier] = item
            else:
                key_id_dict[id(item)] = item

        merged_list = list(key_id_dict.values())
        return merged_list
    else:
        unique_items = set(list1).union(set(list2))
        return list(unique_items)

custom_merger = Merger(
    [
     (list, custom_list_merge),
     (set, "union"),
     (tuple, "concat"),
     (dict, "merge"),
    ],
    ["override"],
    ["override"]
)

def merge_dicts(dicts_list):
    merged_dict = {}
    for d in dicts_list:
        merged_dict = custom_merger.merge(merged_dict, d)
    return merged_dict

def validate_lives_element(element):
    """
    éªŒè¯å•ä¸ª lives å…ƒç´ æ˜¯å¦ç¬¦åˆå†…ç½®é¢‘é“æ¨¡å¼çš„åˆæ³•ç»“æ„
    ç¡®ä¿ä¸ä¼šå¯¼è‡´ loadLives æ–¹æ³•å¼‚å¸¸ä¸­æ–­
    :param element: lives æ•°ç»„ä¸­çš„å•ä¸ªå…ƒç´ 
    :return: bool - æ˜¯å¦åˆæ³•
    """
    # æ£€æŸ¥å…ƒç´ æ˜¯å¦ä¸ºå­—å…¸
    if not isinstance(element, dict):
        print("  [Validate] è·³è¿‡ï¼šéå­—å…¸å…ƒç´ ")
        return False
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦å­—æ®µ
    if 'group' not in element:
        print("  [Validate] è·³è¿‡ï¼šç¼ºå°‘ group å­—æ®µ")
        return False
    
    if 'channels' not in element:
        print("  [Validate] è·³è¿‡ï¼šç¼ºå°‘ channels å­—æ®µ")
        return False
    
    # æ£€æŸ¥ group å­—æ®µæ˜¯å¦ä¸ºéç©ºå­—ç¬¦ä¸²
    if not isinstance(element['group'], str) or not element['group'].strip():
        print("  [Validate] è·³è¿‡ï¼šgroup å­—æ®µä¸ºç©ºæˆ–éå­—ç¬¦ä¸²")
        return False
    
    # æ£€æŸ¥ channels å­—æ®µæ˜¯å¦ä¸ºæ•°ç»„
    if not isinstance(element['channels'], list):
        print("  [Validate] è·³è¿‡ï¼šchannels å­—æ®µéæ•°ç»„")
        return False
    
    # æ£€æŸ¥ channels æ•°ç»„æ˜¯å¦ä¸ºç©º
    if not element['channels']:
        print("  [Validate] è·³è¿‡ï¼šchannels æ•°ç»„ä¸ºç©º")
        return False
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å« proxy://ï¼Œå¦‚æœåŒ…å«åˆ™è§†ä¸ºæ— æ•ˆ
    element_str = json.dumps(element)
    if 'proxy://' in element_str:
        print("  [Validate] è·³è¿‡ï¼šåŒ…å« proxy://")
        return False
    
    # æ£€æŸ¥æ¯ä¸ª channel å…ƒç´ 
    valid_channels = []
    for channel in element['channels']:
        if isinstance(channel, dict) and 'name' in channel and 'urls' in channel:
            if isinstance(channel['name'], str) and channel['name'].strip():
                if isinstance(channel['urls'], list) and channel['urls']:
                    # æ£€æŸ¥ urls æ•°ç»„å…ƒç´ æ˜¯å¦ä¸ºå­—ç¬¦ä¸²
                    valid_urls = []
                    for url in channel['urls']:
                        if isinstance(url, str) and url.strip():
                            valid_urls.append(url)
                    if valid_urls:
                        channel['urls'] = valid_urls
                        valid_channels.append(channel)
    
    if not valid_channels:
        print("  [Validate] è·³è¿‡ï¼šchannels æ•°ç»„ä¸­æ— åˆæ³•é¢‘é“")
        return False
    
    # æ›´æ–°ä¸ºéªŒè¯åçš„ channels
    element['channels'] = valid_channels
    return True


def parse_m3u_content(content):
    """
    è§£æm3uæ ¼å¼å†…å®¹
    :param content: m3uæ–‡ä»¶å†…å®¹
    :return: è½¬æ¢åçš„groupæ ¼å¼åˆ—è¡¨
    """
    try:
        groups = {}
        lines = content.strip().split('\n')
        current_group = 'æœªåˆ†ç»„'
        current_channel = None
        
        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue
            
            if line.startswith('#EXTINF'):
                # æå–åˆ†ç»„å’Œé¢‘é“å
                group_match = re.search(r'group-title="([^"]*)"', line)
                if group_match:
                    current_group = group_match.group(1)
                
                # æå–é¢‘é“å
                name_match = re.search(r',(.+)$', line)
                if name_match:
                    current_channel = name_match.group(1).strip()
                
            elif line.startswith('http') and current_channel:
                # æ·»åŠ URLåˆ°å¯¹åº”é¢‘é“
                if current_group not in groups:
                    groups[current_group] = {}
                
                if current_channel not in groups[current_group]:
                    groups[current_group][current_channel] = []
                
                groups[current_group][current_channel].append(line)
                current_channel = None
        
        # è½¬æ¢ä¸ºgroupæ ¼å¼
        result = []
        for group_name, channels in groups.items():
            group_item = {
                'group': group_name,
                'channels': []
            }
            
            for channel_name, urls in channels.items():
                group_item['channels'].append({
                    'name': channel_name,
                    'urls': urls
                })
            
            result.append(group_item)
        
        return result
    except Exception as e:
        print(f"[Convert] m3uè§£æå¤±è´¥: {e}")
        return None

def parse_txt_content(content):
    """
    è§£ætxtæ ¼å¼å†…å®¹
    :param content: txtæ–‡ä»¶å†…å®¹
    :return: è½¬æ¢åçš„groupæ ¼å¼åˆ—è¡¨
    """
    try:
        groups = {}
        lines = content.strip().split('\n')
        current_group = 'æœªåˆ†ç»„'
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            if line.endswith('#genre#'):
                # æå–åˆ†ç»„å
                current_group = line.replace('#genre#', '').strip()
                # å»é™¤å¯èƒ½å­˜åœ¨çš„æœ«å°¾é€—å·
                if current_group.endswith(','):
                    current_group = current_group[:-1].strip()
                if current_group not in groups:
                    groups[current_group] = {}
            else:
                # æå–é¢‘é“åå’ŒURL
                # åªåœ¨ç¬¬ä¸€ä¸ªé€—å·å¤„åˆ†å‰²ï¼Œå¤„ç†URLä¸­å¯èƒ½åŒ…å«é€—å·çš„æƒ…å†µ
                comma_index = line.find(',')
                if comma_index != -1:
                    channel_name = line[:comma_index].strip()
                    channel_urls_str = line[comma_index+1:].strip()
                    
                    if channel_name and channel_urls_str:
                        # æŒ‰ # åˆ†å‰²å¤šä¸ª URL
                        for url in channel_urls_str.split('#'):
                            url = url.strip()
                            if url and (url.startswith('http') or url.startswith('rtsp') or url.startswith('rtmp')):
                                if current_group not in groups:
                                    groups[current_group] = {}
                                
                                if channel_name not in groups[current_group]:
                                    groups[current_group][channel_name] = []
                                
                                if url not in groups[current_group][channel_name]:
                                    groups[current_group][channel_name].append(url)
        
        # è½¬æ¢ä¸ºgroupæ ¼å¼
        result = []
        for group_name, channels in groups.items():
            group_item = {
                'group': group_name,
                'channels': []
            }
            
            for channel_name, urls in channels.items():
                group_item['channels'].append({
                    'name': channel_name,
                    'urls': urls
                })
            
            result.append(group_item)
        
        return result
    except Exception as e:
        print(f"[Convert] txtè§£æå¤±è´¥: {e}")
        return None



def convert_to_group_format(element):
    """
    å°†éåˆæ³•çš„liveså…ƒç´ è½¬æ¢ä¸ºåˆæ³•çš„groupæ ¼å¼
    :param element: livesæ•°ç»„ä¸­çš„å•ä¸ªå…ƒç´ 
    :return: è½¬æ¢åçš„groupæ ¼å¼å…ƒç´ ï¼Œè½¬æ¢å¤±è´¥è¿”å›None
    """
    if not isinstance(element, dict) or 'url' not in element:
        return None
    
    url = element.get('url', '').strip()
    if not url:
        return None
    
    # æ£€æµ‹URLç±»å‹
    url_lower = url.lower()
    
    if url_lower.endswith('.m3u'):
        # å¤„ç†m3uç±»å‹
        content = get_url_content(url)
        if content:
            return parse_m3u_content(content)
        return None
    
    elif url_lower.endswith('.txt'):
        # å¤„ç†txtç±»å‹ï¼Œæ ¹æ®å†…å®¹åˆ¤æ–­å®é™…æ ¼å¼
        content = get_url_content(url)
        if content:
            # æ ¹æ®å†…å®¹ç‰¹å¾åˆ¤æ–­æ˜¯m3uè¿˜æ˜¯txtæ ¼å¼
            if content.strip().startswith('#EXTM3U'):
                print("[Convert] æ£€æµ‹åˆ°txtåç¼€çš„m3uæ ¼å¼å†…å®¹")
                return parse_m3u_content(content)
            else:
                print("[Convert] æ£€æµ‹åˆ°txtæ ¼å¼å†…å®¹")
                return parse_txt_content(content)
        return None
    
    elif url_lower.endswith('.m3u8'):
        # å¤„ç†m3u8ç±»å‹
        try:
            group_name = element.get('group', 'å…¶ä»–').strip() or 'å…¶ä»–'
            channel_name = element.get('name', 'æœªçŸ¥é¢‘é“').strip() or 'æœªçŸ¥é¢‘é“'
            
            # æ„å»ºç®€å•çš„groupæ ¼å¼
            result = [{
                'group': group_name,
                'channels': [{
                    'name': channel_name,
                    'urls': [url]
                }]
            }]
            
            return result
        except Exception as e:
            print(f"[Convert] m3u8è½¬æ¢å¤±è´¥: {e}")
            return None
    
    return None

def get_most_frequent(stats_dict):
    """
    è·å–å‡ºç°æ¬¡æ•°æœ€å¤šçš„é”®
    :param stats_dict: ç»Ÿè®¡å­—å…¸ {é”®: æ¬¡æ•°}
    :return: å‡ºç°æ¬¡æ•°æœ€å¤šçš„é”®
    """
    if not stats_dict:
        return 'æœªåˆ†ç»„'
    return max(stats_dict.items(), key=lambda x: x[1])[0]

def lives_to_m3u(lives):
    """
    å°† lives æ•°ç»„è½¬æ¢ä¸º m3u æ ¼å¼
    :param lives: lives æ•°ç»„
    :return: m3u æ ¼å¼çš„å­—ç¬¦ä¸²
    """
    if not isinstance(lives, list):
        return ""
    
    m3u_lines = ["#EXTM3U"]
    
    for group_item in lives:
        if not isinstance(group_item, dict):
            continue
        
        group_name = group_item.get('group', 'æœªåˆ†ç»„')
        channels = group_item.get('channels', [])
        
        for channel_item in channels:
            if not isinstance(channel_item, dict):
                continue
            
            channel_name = channel_item.get('name', 'æœªå‘½å')
            urls = channel_item.get('urls', [])
            
            for url in urls:
                if not url:
                    continue
                
                # æ·»åŠ é¢‘é“ä¿¡æ¯
                m3u_lines.append(f"#EXTINF:-1 tvg-name=\"{channel_name}\" group-title=\"{group_name}\",{channel_name}")
                m3u_lines.append(url)
    
    return "\n".join(m3u_lines)

def write_m3u_to_file(m3u_content, file_path):
    """
    å°† m3u å†…å®¹å†™å…¥æ–‡ä»¶
    :param m3u_content: m3u æ ¼å¼çš„å†…å®¹
    :param file_path: æ–‡ä»¶è·¯å¾„
    """
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(m3u_content)
        print(f"M3U content written to: {file_path}")
    except Exception as e:
        print(f"Error writing M3U file {file_path}: {str(e)}")


def lives_to_txt(lives):
    """
    å°† lives æ•°ç»„è½¬æ¢ä¸º TXT æ ¼å¼
    :param lives: lives æ•°ç»„
    :return: TXT æ ¼å¼çš„å­—ç¬¦ä¸²
    """
    txt_lines = []
    
    for group_item in lives:
        if not isinstance(group_item, dict):
            continue
        
        group_name = group_item.get('group', 'æœªåˆ†ç»„')
        channels = group_item.get('channels', [])
        
        # æ·»åŠ åˆ†ç»„å®šä¹‰
        txt_lines.append(f"{group_name},#genre#")
        
        # æ·»åŠ é¢‘é“å®šä¹‰
        for channel_item in channels:
            if not isinstance(channel_item, dict):
                continue
            
            channel_name = channel_item.get('name', 'æœªå‘½å')
            urls = channel_item.get('urls', [])
            
            # å°†å¤šä¸ª URL ç”¨ # è¿æ¥
            if urls:
                # å¯¹æ¯ä¸ª URL ä¸­çš„ # è¿›è¡Œ URL encode ç¼–ç æ›¿æ¢
                encoded_urls = []
                for url in urls:
                    # åªå¯¹ # è¿›è¡Œç¼–ç ï¼Œä¿ç•™å…¶ä»–å­—ç¬¦
                    encoded_url = url.replace('#', '%23')
                    encoded_urls.append(encoded_url)
                urls_str = '#'.join(encoded_urls)
                txt_lines.append(f"{channel_name},{urls_str}")
        
        # æ·»åŠ ç©ºè¡Œåˆ†éš”ä¸åŒåˆ†ç»„
        txt_lines.append('')
    
    return '\n'.join(txt_lines)


def write_txt_to_file(txt_content, file_path):
    """
    å°† txt å†…å®¹å†™å…¥æ–‡ä»¶
    :param txt_content: txt æ ¼å¼çš„å†…å®¹
    :param file_path: æ–‡ä»¶è·¯å¾„
    """
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(txt_content)
        print(f"TXT content written to: {file_path}")
    except Exception as e:
        print(f"Error writing TXT file {file_path}: {str(e)}")

def clean_string(s, keywords):
    """
    æ¸…ç†å­—ç¬¦ä¸²ï¼Œç§»é™¤æŒ‡å®šå…³é”®å­—
    :param s: åŸå§‹å­—ç¬¦ä¸²
    :param keywords: è¦ç§»é™¤çš„å…³é”®å­—åˆ—è¡¨
    :return: æ¸…ç†åçš„å­—ç¬¦ä¸²
    """
    if not isinstance(s, str):
        return s
    cleaned = s
    for keyword in keywords:
        cleaned = cleaned.replace(keyword, '')
    cleaned = cleaned.strip()
    return cleaned if cleaned else 'æœªå‘½å'

def should_exclude_from_aggregation(channel_name):
    """
    åˆ¤æ–­é¢‘é“æ˜¯å¦åº”æ’é™¤åœ¨èšåˆä¹‹å¤–
    :param channel_name: é¢‘é“å
    :return: æ˜¯å¦æ’é™¤
    """
    if not isinstance(channel_name, str):
        return False
    # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯æ•°å­—
    if channel_name.isdigit():
        return True
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ’é™¤å…³é”®å­—
    for keyword in CHANNEL_AGGREGATION_EXCLUDE_KEYWORDS:
        if keyword in channel_name:
            return True
    return False

def custom_channel_sort_key(channel_name):
    """
    è‡ªå®šä¹‰é¢‘é“æ’åºé”®ï¼Œæ”¯æŒå­—ç¬¦ä¸²æ’åºå’Œæœ«å°¾æ•°å­—æ’åº
    :param channel_name: é¢‘é“å
    :return: æ’åºé”®
    """
    if not isinstance(channel_name, str):
        return (channel_name,)
    # æå–æœ«å°¾çš„æ•°å­—éƒ¨åˆ†
    match = re.search(r'(\d+)$', channel_name)
    if match:
        # åˆ†ç¦»å­—ç¬¦ä¸²éƒ¨åˆ†å’Œæ•°å­—éƒ¨åˆ†
        str_part = channel_name[:match.start()]
        num_part = int(match.group(1))
        return (str_part, num_part)
    else:
        # æ²¡æœ‰æ•°å­—éƒ¨åˆ†ï¼Œç›´æ¥è¿”å›å­—ç¬¦ä¸²
        return (channel_name, 0)

def merge_lives_groups(lives):
    """
    åˆå¹¶ lives æ•°ç»„ä¸­çš„é‡å¤åˆ†ç»„å’Œé¢‘é“
    ä½¿ç”¨ URL èšåˆå¹¶ç»Ÿè®¡æ¬¡æ•°çš„ç®—æ³•
    :param lives: lives æ•°ç»„
    :return: åˆå¹¶åçš„ lives æ•°ç»„
    """
    if not isinstance(lives, list):
        return []
    
    # 1. æŒ‰ URL èšåˆå¹¶ç»Ÿè®¡æ¬¡æ•°
    url_to_group_stats = {}  # URL -> {åˆ†ç»„å: å‡ºç°æ¬¡æ•°}
    url_to_channel_stats = {}  # URL -> {é¢‘é“å: å‡ºç°æ¬¡æ•°}
    
    for group_item in lives:
        if not isinstance(group_item, dict):
            continue
        
        # æ¸…æ´—åˆ†ç»„å
        original_group_name = group_item.get('group', 'æœªåˆ†ç»„')
        cleaned_group_name = clean_string(original_group_name, GROUP_NAME_CLEAN_KEYWORDS)
        
        channels = group_item.get('channels', [])
        
        for channel_item in channels:
            if not isinstance(channel_item, dict):
                continue
            
            original_channel_name = channel_item.get('name', 'æœªå‘½å')
            # æ¸…æ´—é¢‘é“åï¼ˆå¯¹æ‰€æœ‰æƒ…å†µéƒ½ç”Ÿæ•ˆï¼‰
            cleaned_channel_name = clean_string(original_channel_name, CHANNEL_NAME_CLEAN_KEYWORDS)
            
            urls = channel_item.get('urls', [])
            
            for url in urls:
                if not url:
                    continue
                
                # æ›´æ–°åˆ†ç»„ç»Ÿè®¡
                if url not in url_to_group_stats:
                    url_to_group_stats[url] = {}
                # å¯¹æ‰€æœ‰æƒ…å†µéƒ½ä½¿ç”¨æ¸…æ´—åçš„åˆ†ç»„å
                url_to_group_stats[url][cleaned_group_name] = url_to_group_stats[url].get(cleaned_group_name, 0) + 1
                
                # æ›´æ–°é¢‘é“ç»Ÿè®¡
                if url not in url_to_channel_stats:
                    url_to_channel_stats[url] = {}
                url_to_channel_stats[url][cleaned_channel_name] = url_to_channel_stats[url].get(cleaned_channel_name, 0) + 1
    
    # 2. ä¸ºæ¯ä¸ª URL é€‰æ‹©å‡ºç°æ¬¡æ•°æœ€å¤šçš„åˆ†ç»„å’Œé¢‘é“
    url_to_best_match = {}
    for url, group_stats in url_to_group_stats.items():
        best_group = get_most_frequent(group_stats)
        channel_stats = url_to_channel_stats.get(url, {})
        best_channel = get_most_frequent(channel_stats)
        url_to_best_match[url] = (best_group, best_channel)
    
    # 3. æŒ‰ç…§é¢‘é“èšåˆç»Ÿè®¡åˆ†ç»„æ¬¡æ•°ï¼Œåˆå¹¶é¢‘é“å¹¶å½’å…¥æ¬¡æ•°æœ€å¤šçš„åˆ†ç»„
    channel_to_group_stats = {}
    channel_to_urls = {}
    excluded_channels = []  # å­˜å‚¨åº”æ’é™¤èšåˆçš„é¢‘é“ [{'channel': channel_name, 'group': group_name, 'urls': [url1, url2, ...]}]
    
    for url, (group, channel) in url_to_best_match.items():
        # æ£€æŸ¥æ˜¯å¦åº”æ’é™¤åœ¨èšåˆä¹‹å¤–
        should_exclude = should_exclude_from_aggregation(channel)
        
        if should_exclude:
            # å¯¹äºåº”æ’é™¤èšåˆçš„é¢‘é“ï¼Œå•ç‹¬ä¿å­˜
            # æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨ç›¸åŒé¢‘é“å’Œåˆ†ç»„çš„è®°å½•
            existing_record = None
            for record in excluded_channels:
                if record['channel'] == channel and record['group'] == group:
                    existing_record = record
                    break
            
            if existing_record:
                # å¦‚æœå·²å­˜åœ¨ï¼Œæ·»åŠ  URL
                if url not in existing_record['urls']:
                    existing_record['urls'].append(url)
            else:
                # å¦‚æœä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°è®°å½•
                excluded_channels.append({'channel': channel, 'group': group, 'urls': [url]})
        else:
            # ç»Ÿè®¡é¢‘é“çš„åˆ†ç»„æ¬¡æ•°
            if channel not in channel_to_group_stats:
                channel_to_group_stats[channel] = {}
            channel_to_group_stats[channel][group] = channel_to_group_stats[channel].get(group, 0) + 1
            
            # æ”¶é›†é¢‘é“çš„æ‰€æœ‰ URL
            if channel not in channel_to_urls:
                channel_to_urls[channel] = []
            if url not in channel_to_urls[channel]:
                channel_to_urls[channel].append(url)
    
    # ä¸ºæ¯ä¸ªé¢‘é“é€‰æ‹©å‡ºç°æ¬¡æ•°æœ€å¤šçš„åˆ†ç»„
    channel_to_best_group = {}
    for channel, group_stats in channel_to_group_stats.items():
        best_group = get_most_frequent(group_stats)
        channel_to_best_group[channel] = best_group
    
    # ä¸éœ€è¦åœ¨è¿™é‡Œåˆå¹¶åº”æ’é™¤èšåˆçš„é¢‘é“ï¼Œå› ä¸ºå®ƒä»¬ä¼šåœ¨æ­¥éª¤ 4 ä¸­å•ç‹¬å¤„ç†
    
    # 4. æ„å»ºåˆ†ç»„-é¢‘é“-URL çš„ç»“æ„
    group_channel_map = {}
    # å¤„ç†ä¸åº”æ’é™¤èšåˆçš„é¢‘é“
    for channel, best_group in channel_to_best_group.items():
        urls = channel_to_urls[channel]
        if best_group not in group_channel_map:
            group_channel_map[best_group] = {}
        if channel not in group_channel_map[best_group]:
            group_channel_map[best_group][channel] = []
        # åˆå¹¶æ‰€æœ‰ URL
        for url in urls:
            if url not in group_channel_map[best_group][channel]:
                group_channel_map[best_group][channel].append(url)
    
    # å¤„ç†åº”æ’é™¤èšåˆçš„é¢‘é“ï¼Œä¿æŒåŸåˆ†ç»„
    for record in excluded_channels:
        channel = record['channel']
        group = record['group']
        urls = record['urls']
        
        if group not in group_channel_map:
            group_channel_map[group] = {}
        if channel not in group_channel_map[group]:
            group_channel_map[group][channel] = []
        # åˆå¹¶æ‰€æœ‰ URL
        for url in urls:
            if url not in group_channel_map[group][channel]:
                group_channel_map[group][channel].append(url)
    
    # 5. å¦‚æœåˆ†ç»„ä¸‹åªæœ‰1ä¸ªé¢‘é“ï¼Œä¸”åˆ†ç»„å’Œé¢‘é“åç›¸åŒçš„ï¼Œåˆ™å°†è¿™äº›éƒ½åˆå¹¶åˆ°ä¸€ä¸ªåˆ†ç»„ä¸­ï¼Œåˆ†ç»„å"å•å‰§"ï¼Œé¢‘é“åä½¿ç”¨åŸé¢‘é“å
    single_drama_group = "å•å‰§"
    group_channel_map[single_drama_group] = {}
    
    # æ”¶é›†éœ€è¦ç§»åŠ¨çš„é¢‘é“
    channels_to_move = []
    for group_name, channels in group_channel_map.items():
        if group_name == single_drama_group:
            continue
        if len(channels) == 1:
            channel_name = list(channels.keys())[0]
            if group_name == channel_name:
                channels_to_move.append((channel_name, channels[channel_name]))
    
    # ç§»åŠ¨é¢‘é“åˆ°"å•å‰§"åˆ†ç»„
    for channel_name, urls in channels_to_move:
        # ä»åŸåˆ†ç»„ä¸­ç§»é™¤
        for group_name, channels in list(group_channel_map.items()):
            if channel_name in channels:
                del channels[channel_name]
                # å¦‚æœåˆ†ç»„ä¸ºç©ºï¼Œåˆ™åˆ é™¤åˆ†ç»„
                if not channels:
                    del group_channel_map[group_name]
        # æ·»åŠ åˆ°"å•å‰§"åˆ†ç»„
        if channel_name not in group_channel_map[single_drama_group]:
            group_channel_map[single_drama_group][channel_name] = []
        for url in urls:
            if url not in group_channel_map[single_drama_group][channel_name]:
                group_channel_map[single_drama_group][channel_name].append(url)
    
    # 6. æŒ‰ç…§è‡ªå®šä¹‰è§„åˆ™æ’åºåˆ†ç»„ï¼Œåˆ†ç»„å†…æŒ‰ç…§é¢‘é“åé¡ºå‘æ’åº
    # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼å¹¶æ’åº
    merged_lives = []
    
    # è®¡ç®—æ¯ä¸ªåˆ†ç»„çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œç”¨äºæ’åº
    group_stats = []
    for group_name, channels in group_channel_map.items():
        if not channels:
            continue
        channel_count = len(channels)
        url_count = sum(len(urls) for urls in channels.values())
        # è®¡ç®—æ¯”å€¼
        ratio = url_count / channel_count if channel_count > 0 else 0
        group_stats.append((group_name, channels, channel_count, url_count, ratio))
    
    # è‡ªå®šä¹‰æ’åºè§„åˆ™
    def custom_sort_key(item):
        group_name, channels, channel_count, url_count, ratio = item
        if channel_count > 10:
            # é¢‘é“æ•°>10ï¼šæ’åœ¨å‰é¢åŒºåŸŸï¼ŒæŒ‰URLæ•°/é¢‘é“æ•°çš„æ¯”å€¼ä»å¤§åˆ°å°æ’åºï¼Œç›¸åŒæ—¶æŒ‰é¢‘é“æ•°é™åº
            return (0, -ratio, -channel_count, group_name)
        else:
            # é¢‘é“æ•°<=10ï¼šæ’åœ¨åé¢åŒºåŸŸï¼ŒæŒ‰é¢‘é“æ•°ä»å¤šåˆ°å°‘æ’åº
            return (1, -channel_count, 0.0, group_name)
    
    # æŒ‰è‡ªå®šä¹‰è§„åˆ™æ’åº
    sorted_groups = sorted(group_stats, key=custom_sort_key)
    
    for group_name, channels, channel_count, url_count, ratio in sorted_groups:
        # è·³è¿‡ç©ºåˆ†ç»„
        if not channels:
            continue
        
        # æŒ‰ç…§é¢‘é“åé¡ºå‘æ’åºï¼Œæ”¯æŒå­—ç¬¦ä¸²å’Œæœ«å°¾æ•°å­—æ’åº
        sorted_channels = sorted(channels.items(), key=lambda x: custom_channel_sort_key(x[0]))
        
        merged_channels = []
        for channel_name, urls in sorted_channels:
            merged_channels.append({
                'name': channel_name,
                'urls': urls
            })
        
        merged_lives.append({
            'group': group_name,
            'channels': merged_channels
        })
    
    return merged_lives

def validate_lives(lives, output_m3u_path=None, output_txt_path=None):
    """
    éªŒè¯å¹¶æ¸…ç† lives æ•°ç»„
    :param lives: lives æ•°ç»„
    :param output_m3u_path: m3u è¾“å‡ºæ–‡ä»¶è·¯å¾„
    :param output_txt_path: txt è¾“å‡ºæ–‡ä»¶è·¯å¾„
    :return: éªŒè¯åçš„ lives æ•°ç»„
    """
    if not isinstance(lives, list):
        print("[Validate] lives éæ•°ç»„ï¼Œåˆå§‹åŒ–ä¸ºç©ºæ•°ç»„")
        return []
    
    valid_lives = []
    for element in lives:
        if validate_lives_element(element):
            valid_lives.append(element)
        else:
            # å°è¯•è½¬æ¢ä¸ºgroupæ ¼å¼
            print("[Validate] å°è¯•è½¬æ¢éåˆæ³•å…ƒç´ ä¸ºgroupæ ¼å¼")
            converted = convert_to_group_format(element)
            if converted and isinstance(converted, list):
                print(f"[Validate] è½¬æ¢æˆåŠŸï¼Œæ·»åŠ  {len(converted)} ä¸ªgroupå…ƒç´ ")
                valid_lives.extend(converted)
            elif converted:
                print("[Validate] è½¬æ¢æˆåŠŸï¼Œæ·»åŠ 1ä¸ªgroupå…ƒç´ ")
                valid_lives.append(converted)
            else:
                print("[Validate] è½¬æ¢å¤±è´¥ï¼Œè·³è¿‡è¯¥å…ƒç´ ")
    
    # åˆå¹¶ç»“æœ
    merged_lives = merge_lives_groups(valid_lives)
    print(f"[Validate] lives åˆå¹¶å®Œæˆï¼šä» {len(valid_lives)} ä¸ªå…ƒç´ åˆå¹¶ä¸º {len(merged_lives)} ä¸ªå…ƒç´ ")
    
    # è½¬æ¢ä¸ºm3uæ ¼å¼å¹¶è¾“å‡º
    if output_m3u_path:
        m3u_content = lives_to_m3u(merged_lives)
        write_m3u_to_file(m3u_content, output_m3u_path)
    
    # è½¬æ¢ä¸ºtxtæ ¼å¼å¹¶è¾“å‡º
    if output_txt_path:
        txt_content = lives_to_txt(merged_lives)
        write_txt_to_file(txt_content, output_txt_path)
    
    print(f"[Validate] lives éªŒè¯å®Œæˆï¼šå…±å¤„ç† {len(lives)} ä¸ªå…ƒç´ ï¼Œç”Ÿæˆ {len(merged_lives)} ä¸ªæœ‰æ•ˆgroupå…ƒç´ ")
    return merged_lives


def validate_sites(sites):
    """
    éªŒè¯å¹¶æ¸…ç† sites æ•°ç»„
    :param sites: sites æ•°ç»„
    :return: éªŒè¯åçš„ sites æ•°ç»„
    """
    if not isinstance(sites, list):
        print("[Validate] sites éæ•°ç»„ï¼Œåˆå§‹åŒ–ä¸ºç©ºæ•°ç»„")
        return []
    
    valid_sites = []
    for site in sites:
        if isinstance(site, dict) and all(field in site for field in SITES_REQUIRED_FIELDS):
            valid_sites.append(site)
    
    print(f"[Validate] sites éªŒè¯å®Œæˆï¼š{len(valid_sites)}/{len(sites)} ä¸ªå…ƒç´ æœ‰æ•ˆ")
    return valid_sites


def write_json_to_file(data, file_path=OUTPUT_FILE_PATH):
    try:
        with open(file_path, 'w', encoding='utf-8') as output_file:
            json.dump(data, output_file, indent=4, ensure_ascii=False)
        print(f"Data written to JSON file: {file_path}")
    except Exception as e:
        print(f"Error writing data to JSON file {file_path}: {str(e)}")

# ================= [æ–°å¢] æ·±åº¦é€’å½’æ›¿æ¢ç›¸å¯¹è·¯å¾„å‡½æ•° ==================
def deep_replace_relative_paths(obj, base_url):
    """
    æ·±åº¦é€’å½’éå†å¯¹è±¡ï¼Œæ›¿æ¢å­—å…¸ä¸­ä»¥ "./" å¼€å¤´çš„å­—ç¬¦ä¸²å€¼
    :param obj: å½“å‰éå†çš„å¯¹è±¡ (dict/list/str)
    :param base_url: ç”¨äºæ‹¼æ¥çš„åŸºå‡† URL
    """
    # éœ€æ±‚ 1ï¼šå¦‚æœæ˜¯æœ¬åœ°æ–‡ä»¶æºï¼Œä¸åšå¤„ç†
    if base_url.startswith((".", "/")):
        return

    if isinstance(obj, dict):
        for key, value in obj.items():
            if isinstance(value, str):
                # éœ€æ±‚ 3ï¼šå€¼ä¸ºå­—ç¬¦ä¸²ä¸”ä»¥ "./" å¼€å¤´
                if value.startswith("./"):
                    # éœ€æ±‚ 4ï¼šä½¿ç”¨ urljoin è¿›è¡Œæ ‡å‡†æ‹¼æ¥
                    obj[key] = urljoin(base_url, value)
            else:
                # é€’å½’å¤„ç†ä¸‹ä¸€å±‚ï¼ˆä¸é™åˆ¶æ·±åº¦ï¼‰
                deep_replace_relative_paths(value, base_url)
    elif isinstance(obj, list):
        # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œéå†å…¶ä¸­çš„å…ƒç´ ç»§ç»­é€’å½’
        for item in obj:
            deep_replace_relative_paths(item, base_url)
# =================================================================

def add_original_url(url, d):
    if 'originalUrl' not in d:
        d['originalUrl'] = []
    if isinstance(d['originalUrl'], str):
        d['originalUrl'] = [d['originalUrl'], url]
    else:
        d['originalUrl'].append(url)

def preprocess_single_dict(url, d):
    """
    é’ˆå¯¹å•ä¸ªå•ä»“å­—å…¸è¿›è¡Œé¢„å¤„ç†
    """
    add_original_url(url, d)

    # éœ€æ±‚ 2ï¼šä¸å†å•ç‹¬å¤„ç† spider å­—æ®µ

    # éœ€æ±‚ 3ï¼šå¤„ç†é¡¶çº§ sites ä¸‹çš„å­—æ®µ
    if "sites" in d:
        # ç¡®å®šåŸºå‡† URLï¼šä¼˜å…ˆä½¿ç”¨ originalUrl ä¸­çš„ç¬¬ä¸€ä¸ªï¼Œå¦åˆ™ä½¿ç”¨å½“å‰ urlï¼ˆå¯¹äºæœ¬åœ°æ–‡ä»¶ä¹Ÿå¯ä»¥é€šè¿‡è¯¥æ–¹å¼è¿›è¡Œæ­£ç¡®æ›¿æ¢ï¼‰
        base_url_for_replace = url
        if "originalUrl" in d and isinstance(d["originalUrl"], list) and d["originalUrl"]:
            first_original_url = d["originalUrl"][0]
            if first_original_url and not first_original_url.startswith((".", "/")):
                base_url_for_replace = first_original_url

        # æ‰§è¡Œæ·±åº¦æ›¿æ¢
        deep_replace_relative_paths(d["sites"], base_url_for_replace)

# ================= [æ–°å¢] åŠ è½½é»˜è®¤è¦†ç›–æ–‡ä»¶çš„å‡½æ•° =================
def load_override_file(file_path):
    """
    åŠ è½½å¹¶æ ¡éªŒè¦†ç›–æ–‡ä»¶
    :param file_path: è¦†ç›–æ–‡ä»¶è·¯å¾„
    :return: dict or None
    """
    p = Path(file_path)
    if not p.exists():
        print(f"[Override] æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œè·³è¿‡è¦†ç›–ã€‚")
        return None

    print(f"[Override] å‘ç°è¦†ç›–æ–‡ä»¶: {file_path}ï¼Œæ­£åœ¨åŠ è½½...")
    content = get_local_file_content(file_path)

    if content is None:
        print(f"[Override] æ–‡ä»¶ {file_path} è¯»å–å¤±è´¥æˆ–ä¸ºç©ºï¼Œè·³è¿‡è¦†ç›–ã€‚")
        return None

    # æ¸…æ´—æ³¨é‡Š
    content = remove_comments_from_string(content)
    content = content.replace("\n", "").replace("\r", "")

    if not is_json(content):
        print(f"[Override] æ–‡ä»¶ {file_path} ä¸æ˜¯åˆæ³•çš„ JSONï¼Œè·³è¿‡è¦†ç›–ã€‚")
        return None

    try:
        parsed = json.loads(content)
        if isinstance(parsed, dict):
            # æ³¨æ„ï¼šOverride æ–‡ä»¶æ˜¯æœ¬åœ°æ–‡ä»¶ï¼Œä¼ å…¥ url="" æˆ–ç©ºï¼Œ
            # deep_replace_relative_paths å†…éƒ¨ä¼šè¯†åˆ«æœ¬åœ°è·¯å¾„ä»è€Œè·³è¿‡å¤„ç†ï¼Œ
            # ä½†ä¸ºäº†ä¿é™©ï¼Œè¿™é‡Œå¯ä»¥ä¸è°ƒç”¨ preprocess_single_dictï¼Œ
            # æˆ–è€…ä»…è°ƒç”¨ add_original_urlã€‚
            # è¿™é‡Œé€‰æ‹©ä»…åšæœ€ç®€å•çš„å¤„ç†ï¼Œå› ä¸º Override é€šå¸¸æ˜¯æœ€ç»ˆç»“æœï¼Œä¸éœ€è¦å†è§£æç›¸å¯¹è·¯ å¾„ã€‚
            print(f"[Override] æ–‡ä»¶ {file_path} åŠ è½½æˆåŠŸï¼Œå°†åœ¨æœ€ååˆå¹¶ä»¥è¦†ç›–å‚æ•°ã€‚")
            return parsed
        else:
            print(f"[Override] æ–‡ä»¶ {file_path} JSON æ ¹èŠ‚ç‚¹ä¸æ˜¯ Object (dict)ï¼Œè·³è¿‡è¦†ç›–ã€‚")
            return None
    except Exception as e:
        print(f"[Override] è§£ææ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        return None

# ä¸»å‡½æ•°
if __name__ == "__main__":
    input_file_path = INPUT_FILE_PATH
    current_time = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    output_file_path = current_time + "-" + OUTPUT_FILE_PATH
    output_m3u_path = current_time + "-" + DEFAULT_OUTPUT_M3U_FILE
    output_txt_path = current_time + "-" + DEFAULT_OUTPUT_TXT_FILE

    if len(sys.argv) > 1:
        input_file_path = sys.argv[1]
    if len(sys.argv) > 2:
        output_file_path = sys.argv[2]
    if len(sys.argv) > 3:
        output_m3u_path = sys.argv[3]
    if len(sys.argv) > 4:
        output_txt_path = sys.argv[4]

    # 1. å¤„ç†è¾“å…¥ï¼Œè·å–åŸå§‹æ•°æ®
    raw_data_map, valid_sources, invalid_sources = process_input_file(input_file_path)

    # åˆ©ç”¨ Pathlib å¤„ç†æ–‡ä»¶å
    p = Path(input_file_path)
    p2 = Path(output_file_path)
    filename = p.name
    single_file_path = p2.parent / f"{filename}.single"
    multi_file_path = p2.parent / f"{filename}.multi"
    
    # å®šä¹‰æ–°ç”Ÿæˆçš„æ–‡ä»¶å
    tmp_valid_path = p.parent / f"tmp.{filename}.valid-json"
    invalid_history_path = p2.parent / f"{filename}.invalid-json-history"
    
    # æå‰è¯»å–æ— æ•ˆå†å²æ–‡ä»¶ï¼Œç”¨äºåç»­è¿‡æ»¤
    invalid_history_set = set()
    if invalid_history_path.exists():
        try:
            with open(invalid_history_path, 'r', encoding='utf-8') as f:
                for l in f:
                    stripped = l.strip()
                    if stripped:
                        invalid_history_set.add(stripped)
            print(f"[Info] Loaded {len(invalid_history_set)} invalid history entries")
        except Exception as e:
            print(f"Warning: Could not read invalid history file {invalid_history_path}: {e}")

    # æ”¶é›†æ‰€æœ‰è¾“å…¥çš„ URLï¼ˆç”¨äºè¿‡æ»¤ï¼‰
    all_input_urls = set(raw_data_map.keys())

    # 2. åˆ†ç±»å•ä»“ä¸å¤šä»“ï¼Œå¹¶æ”¶é›†æœ€ç»ˆå¾…åˆå¹¶åˆ—è¡¨
    final_dicts_to_merge = []
    single_urls = []
    multi_urls = []

    print("\n" + "="*30)
    print("Starting Classification & Deep Scan")
    print("="*30)

    for url, data in raw_data_map.items():
        if is_single_cang(data):
            print(f"[Single] {url}")
            single_urls.append(url)
            # é¢„å¤„ç†å¹¶åŠ å…¥åˆå¹¶é˜Ÿåˆ—
            preprocess_single_dict(url, data)
            final_dicts_to_merge.append(data)
        else:
            print(f"[Multi]  {url} -> Starting deep scan...")
            multi_urls.append(url)
            # åŠ å…¥æœ‰æ•ˆæºåˆ—è¡¨ï¼Œç¡®ä¿å¤šä»“URLä¼šè¢«å†™å…¥åˆ°è¾“å…¥æ–‡ä»¶
            valid_sources.append(url)
            # æ·±åº¦éå†æå– URL
            extracted_sub_urls = extract_urls_deep(data)
            # å»é‡
            extracted_sub_urls = list(dict.fromkeys(extracted_sub_urls))

            print(f"  Found {len(extracted_sub_urls)} potential URLs.")

            # è¿‡æ»¤ URL
            filtered_sub_urls = []
            for sub_url in extracted_sub_urls:
                # æ£€æŸ¥æ˜¯å¦åœ¨è¾“å…¥æ–‡ä»¶çš„ URL ä¸­å­˜åœ¨
                if sub_url in all_input_urls:
                    print(f"  [Filter] Skipping URL (exists in input): {sub_url}")
                    continue
                # æ£€æŸ¥æ˜¯å¦åœ¨æ— æ•ˆå†å²æ–‡ä»¶ä¸­å­˜åœ¨
                if sub_url in invalid_history_set:
                    print(f"  [Filter] Skipping URL (exists in invalid history): {sub_url}")
                    continue
                # é€šè¿‡è¿‡æ»¤ï¼Œæ·»åŠ åˆ°å¤„ç†åˆ—è¡¨
                filtered_sub_urls.append(sub_url)
            
            print(f"  After filtering: {len(filtered_sub_urls)} URLs to process")

            # å°è¯•è§£ææ¯ä¸€ä¸ªè¿‡æ»¤åçš„ URL
            for sub_url in filtered_sub_urls:
                sub_data = fetch_and_parse_single_cang(sub_url)
                if sub_data:
                    print(f"  [OK] Resolved as singleä»“: {sub_url}")
                    preprocess_single_dict(sub_url, sub_data)
                    final_dicts_to_merge.append(sub_data)
                    # åŠ å…¥å•ä»“URLåˆ—è¡¨ï¼Œè§†åŒè¾“å…¥æ–‡ä»¶ä¸­çš„å•ä»“å¤„ç†
                    single_urls.append(sub_url)
                    # åŠ å…¥æœ‰æ•ˆæºåˆ—è¡¨ï¼Œç¡®ä¿ä¼šè¢«å†™å…¥åˆ°ä¸´æ—¶æœ‰æ•ˆæ–‡ä»¶
                    valid_sources.append(sub_url)
                else:
                    print(f"  [SKIP] Not valid JSON or not dict: {sub_url}")
                    # åŠ å…¥æ— æ•ˆæºåˆ—è¡¨ï¼Œè§†åŒè¾“å…¥æ–‡ä»¶ä¸­çš„æ— æ•ˆå¤„ç†
                    invalid_sources.append(sub_url)

    # ================= [ä¿®æ”¹] åŠ è½½è¦†ç›–æ–‡ä»¶ï¼Œæ·»åŠ åˆ°å¾…åˆå¹¶åˆ—è¡¨æœ€å =================
    override_data = load_override_file(DEFAULT_OVERRIDE_FILE)
    if override_data:
        print(f"[Override] Adding override data to merge list")
        final_dicts_to_merge.append(override_data)
    # ==========================================================

    # 3. å†™å…¥åˆ†ç±»æ–‡ä»¶
    write_list_to_file(single_file_path, single_urls)
    write_list_to_file(multi_file_path, multi_urls)

    # 4. åˆå¹¶æ‰€æœ‰å­—å…¸ï¼ˆåŒ…å«overrideï¼‰
    print("\n" + "="*30)
    print(f"Merging {len(final_dicts_to_merge)} singleä»“ data...")
    print("="*30)
    final_merged_dict = merge_dicts(final_dicts_to_merge)

    # 6. éªŒè¯å¹¶æ¸…ç† lives æ•°ç»„
    if 'lives' in final_merged_dict:
        print("\n" + "="*30)
        print("Validating lives array")
        print("="*30)
        final_merged_dict['lives'] = validate_lives(final_merged_dict['lives'], output_m3u_path, output_txt_path)
        
        # æ£€æŸ¥ override æ–‡ä»¶æ˜¯å¦å­˜åœ¨é¡¶å±‚ lives å­—æ®µ
        if override_data and 'lives' in override_data:
            print("[Override] Using lives from override file instead of merged result")
            final_merged_dict['lives'] = override_data['lives']

    # 7. éªŒè¯å¹¶æ¸…ç† sites æ•°ç»„
    if 'video' in final_merged_dict and 'sites' in final_merged_dict['video']:
        print("\n" + "="*30)
        print("Validating sites array in video")
        print("="*30)
        final_merged_dict['video']['sites'] = validate_sites(final_merged_dict['video']['sites'])
    elif 'sites' in final_merged_dict:
        print("\n" + "="*30)
        print("Validating sites array")
        print("="*30)
        final_merged_dict['sites'] = validate_sites(final_merged_dict['sites'])

    # 8. åˆ é™¤å¤šä½™é¡¶å±‚å­—æ®µ
    print("\n" + "="*30)
    print("Removing extra top-level fields")
    print("="*30)
    removed_fields = []
    for field in EXTRA_FIELDS:
        if field in final_merged_dict:
            del final_merged_dict[field]
            removed_fields.append(field)
    if removed_fields:
        print(f"Removed fields: {', '.join(removed_fields)}")
    else:
        print("No extra fields found")

    # 9. å†™å…¥ JSON ç»“æœæ–‡ä»¶
    write_json_to_file(final_merged_dict, output_file_path)

    # ================= åŸæœ‰æ–‡ä»¶æ›´æ–°é€»è¾‘ =================

    # è®°å½•æœ‰æ•ˆçš„ JSON æºåˆ° tmp æ–‡ä»¶
    write_list_to_file(tmp_valid_path, valid_sources)

    # è®°å½•æ— æ•ˆçš„ JSON æºåˆ°å†å²æ–‡ä»¶ï¼ˆå»é‡è¿½åŠ ï¼‰
    for src in invalid_sources:
        append_to_file_unique(invalid_history_path, src, invalid_history_set)
        # æ›´æ–°æ— æ•ˆå†å²é›†åˆ
        invalid_history_set.add(src)

    # ä½¿ç”¨ tmp.valid-json è¦†ç›–åŸè¾“å…¥æ–‡ä»¶
    replace_file(tmp_valid_path, input_file_path)
